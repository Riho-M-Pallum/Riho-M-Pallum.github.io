---
project:
  type: website
  output-dir: docs
title: "Robust Bayesian Inference for Set-Identified Models"
format: 
  revealjs:
    slide-number: true
    show-slide-number: print
    incremental: true
    toc: true
    toc-depth: 1
output:
  revealjs:
    self-contained: false
    
editor: visual
---

```{r, echo = F}
library(tidyverse)
library(dplyr)
library(ggplot2)

```

# Quick overview of Bayesian statistics

## Bayes rule

$$
P(\text{Hypothesis} | \text{Data}) =\frac{P(\text{Data} | \text{Hypothesis}) P(\text{Hypothesis})}{P(\text{data})}
$$
- Prior: Our Hyptothesis (= parameters) before seeing the data, set by the researcher

- Likelihood: The probability of observing the data given the Hypothesis, our "model"

- Posterior: Belief about the Hypothesis (= parameters) given the data, the point of Bayesian statistics

  - Evidence: (Usually unobserved) normalising constant

---

### Example model, Bayesian linear regression {.smaller}

Given we chose the model $$ y = \theta_{0} + \theta_{1}x + \epsilon\quad y,x,\theta_{0},\theta_{1} \in \mathbb{R}\\ \epsilon \sim \mathcal{N}(0,\sigma^{2}) $$ We set relatively uninformative priors $$ \theta = \begin{pmatrix}\theta_{0}\\ \theta_{1}\end{pmatrix} \sim \mathcal{N}\left(\begin{pmatrix}0\\0\end{pmatrix},\begin{pmatrix}\sigma_{0}^{2} & 0\\ 0 & \sigma_{1}^{2}\end{pmatrix}\right) = \mathcal{N}(\mu_{prior}, \Sigma_{prior}),\quad \sigma^{2}\sim \text{Inv-Gamma}(\alpha, \beta)$$ The likelihood defined by the model given that we have $k$ samples follows the distribution $$ y|\beta_{0},\beta_{1} \sim \mathcal{N}_{k}(X\beta, diag(\sigma^{2})), \quad X = \begin{pmatrix}1 & x_{1}\\ ... &...\\ 1 & x_{k}\end{pmatrix}$$ Therefore the posterior is $$ P((\theta_{0}, \theta_{1})^{T} | y) \propto \mathcal{N}\left(\Sigma_{posterior}\frac{1}{\sigma^{2}}X^{T}y, (\Sigma_{prior}^{-1} + \frac{1}{\sigma^{2}}X^{T}X)^{-1} \right)$$ !!!! Note the above is false, do not bother with the full distribution

---

## Inference: credible intervals {.smaller}

-   Say, given \textcolor{red}{any sample} $Y^{n} = (y_{1},...,y_{n})$ the posterior $ P(\theta | Y^{n}) $ looks approximately like

-   
    ```{r, echo = F}
    #| fig-width: 5
    #| fig-height: 4
    # Set seed for reproducibility
    set.seed(123)

    # Number of samples from each normal distribution
    n1 <- 1000
    n2 <- 1000

    # Generate data from two normal distributions
    data1 <- rnorm(n1, mean = 5, sd = 1)  # First mode: mean 5, sd 1
    data2 <- rnorm(n2, mean = 10, sd = 1) # Second mode: mean 10, sd 1

    joint_data <- c(data1, data2) |> sort()
    credible_interval <- quantile(joint_data, probs = c(0.025, 0.975))
    cdf_values <- seq_along(joint_data) / length(joint_data)
    x_95 <- joint_data[which.min(abs(cdf_values - 0.95))]
    density_data <- density(joint_data)

    # Create a data frame with the density values
    density_df <- data.frame(x = density_data$x, y = density_data$y)

    # Plot the joint distribution and the 95% credible interval
    density_df |> ggplot( aes(x = x, y = y)) +
      geom_line(color = "darkblue", size = 1.5) +  # Density line
      geom_ribbon(data = density_df[density_df$x >= credible_interval[1] & density_df$x <= credible_interval[2], ],
                  aes(x = x, ymin = 0, ymax = y), fill = "blue", alpha = 0.5) +  # Credible interval
      labs(title = "Bimodal Distribution with two 95% Credible Interval",
           x = "Value", y = "Density") +
      geom_ribbon(data = density_df[density_df$x <= x_95, ], 
                  aes(x = x, ymin = 0, ymax = y), 
                  fill = "red", alpha = 0.5) +

      theme_minimal()
    ```

-   Non unique 95% credible interval

-   Usual workaround of minimising width of interval

---

## Set identification 

-   A parameter set $\Theta$ is said to be (point) identified if $\Theta = \{\theta\}$

-   In a Bayesian setting parameters are set identified if $\exists \theta_{0}, \theta_{1} \in \Theta ~ s.t. \theta_{0} \neq \theta_{1},\quad p(y | \theta_{0}) = p(y | \theta_{1}) ~ \forall y \in Y$

-  I.e. for any draw of the data the priors must be equal, but not necessarily the posteriors.

- Indeed the problem is that the posteriors can be different depending on the draw, even though the priors are the same

::: {.notes}
  Q: but can we not just get rid of the flat region since we choose the prior? A: Yes and we will discuss that in    the next slide
:::


# The main problem and solution in the paper

---

## The problem with set identified models {.smaller}
- Let $\mathbf{Y} \subseteq\mathbb{R}^{d}$ be the set of samples $Y$ and $\Theta\subseteq\mathbb{R}^{d}$ be the set of possible parameter values $\theta$.

-   Suppose we only observe a reduced form parameter $\phi = g(\theta)$, where $g: \Theta \to \Phi,\quad g(\theta) = g(\theta^{'}) \text{ iff }p(Y|\theta) = p(Y|\theta^{'}) \forall Y\in \mathbf{Y}$. Note in set identification $g$ is not injective.

- Define the identified set $IS_{\theta}(\phi) = \{\theta \in \Theta | g(\theta) = \phi\}$

-   In set identified structural models the prior $\pi_{\theta|Y}$ can be decomposed as $\pi_{\theta|Y}(\cdot) = \int_{\Phi}\pi_{\theta |\phi}(\cdot)d\pi_{\phi|Y}$.

-   $\pi_{\phi |Y}$ is updated as data comes in, $\pi_{\theta | \phi}$ is not.

-   We could also choose a $\pi_{\theta |\phi}$, but a good one is often not available. Bypassing this problem is the main contribution of this paper

-   Any valid conditional prior must satisfy $\pi_{\theta |\phi}(IS_{\theta}(\phi)) = 1, \pi_{\phi}\text{-a.s.}$

::: {.notes}
$\Phi$ is just the set of reduced form parameters. Note that g partitons $\Theta$ into equivalence classes and the set of identified sets is the partition induced by $g$. The decomposition of the prior into reduced form and structural given reduced form priors is what creates the flat region. Technically the above image is not correct as we can put any kind of prior on the flat region.
:::

---

### Set identified example
```{r}
# Load ggplot2 library
library(ggplot2)

# Create data for the three colored sections (top rectangle divided into thirds)
sections_data <- data.frame(
  xmin = c(0.5, 1.5, 2.5),
  x = c(1,2,3),
  xmax = c(1.5, 2.5, 3.5),
  ymin = rep(2.5, 3),  # Top rectangle's y-range (2.5 to 3.5)
  ymax = rep(3.5, 3),
  y = rep(3, 3),
  color = c("lightblue", "lightgreen", "lightpink"),  # Different colors for each third
  label = c("ISθ(1)", "ISθ(2)", "ISθ(3)")
)

# Create data for the numbers (bottom rectangle)
numbers_data <- data.frame(
  x = c(1, 2, 3),    # x-coordinates for numbers (matching sections)
  y = rep(1, 3),     # y-coordinate for all numbers (bottom of the plot)
  label = c("{1}", "{2}", "{3}")  # Set notation for the numbers
)

# Create a data frame for the connections (lines)
lines_data <- data.frame(
  x_start = c(1, 2, 3),
  y_start = rep(3, 3),  # Starting y-coordinate for top sections
  x_end = c(1, 2, 3),
  y_end = rep(1, 3)     # Ending y-coordinate for numbers
)

# Create the plot
ggplot() +
  # Add the three colored sections at the top (top rectangle divided into thirds)
  geom_rect(data = sections_data, aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax, fill = color),
            color = "black") +
  geom_text(data = sections_data, aes(x = x, y = y, label = label), vjust = -1, size = 5)+
  annotate("text", x = 0.3, y = 3, label = expression(pi[theta ~ "|" ~ phi]), size = 6, fontface = "italic")+
  
  # Add numbers for the bottom part
  geom_text(data = numbers_data, aes(x = x, y = y, label = label), vjust = 1.5, size = 5) +
  
  # Add lines connecting each section to its corresponding number
  geom_segment(data = lines_data, aes(x = x_start, y = y_start, xend = x_end, yend = y_end), 
               linetype = "solid") +
  annotate("text", x = 0.8, y = 2, label = expression(g^{-1} * "({" * 1 * "})"), size = 5, fontface = "italic")+
  
  annotate("text", x = 1.8, y = 2, label = expression(g^{-1} * "({" * 2 * "})"), size = 5, fontface = "italic")+
  annotate("text", x = 2.8, y = 2, label = expression(g^{-1} * "({" * 3 * "})"), size = 5, fontface = "italic")+
  
  # Add a rectangle around the sections (top rectangle, theta)
  geom_rect(aes(xmin = 0.5, xmax = 3.5, ymin = 2.5, ymax = 3.5), 
            color = "black", fill = NA, linetype = "solid") +
  annotate("text", x = 3.7, y = 3, label = expression(Theta), size = 5, fontface = "bold") +
  
  # Add a rectangle around the numbers (bottom rectangle, phi)
  geom_rect(aes(xmin = 0.5, xmax = 3.5, ymin = 0.5, ymax = 1.5), 
            color = "black", fill = NA, linetype = "solid") +
  annotate("text", x = 3.7, y = 1, label = expression(Phi), size = 5, fontface = "bold") +
  annotate("text", x = 0.3, y = 1, label = expression(pi[phi ~ "|" ~ Y]), size = 6, fontface = "italic")+
  
  # Set axis limits and remove axis labels
  xlim(0, 4) + ylim(0, 4) +
  scale_fill_identity() +  # Use the colors specified in the data
  theme_void()  # Remove background, grid, and axes for a clean look
```

---

### Possible unrevisable priors on the identified sets
```{r, echo = F}
# Simulate some data with an interval
set.seed(123)

# Simulate some data
x_values <- seq(4,9, length.out = 100)
crazy_values <- seq(0, 10, length.out = 100)
crazy_values <- crazy_values/sum(crazy_values)*100
truncated_normal <- dnorm(x_values, mean = 7, sd = 2)
truncated_normal <- truncated_normal/sum(truncated_normal)*100

# Create a data frame
data <- tibble(
  x = x_values,
  crazy_values = crazy_values,
  truncated_normal_values = truncated_normal
)

# Define the x-axis interval you want to highlight (e.g., from x = 3 to x = 7)

# Plot the line and highlight the interval on the x-axis
plot <- data |> ggplot() +
  # Main axis: horizontal line
  geom_segment(aes(x = 0, xend = 12, y = 0, yend = 0), size = 1) +
  
  # Vertical line for x (point on axis)
  geom_segment(aes(x = 4, xend = 4, y = -2, yend = 2), color = "blue", size = 1) +
  
  # Vertical line for x + a (point on axis)
  geom_segment(aes(x = 9, xend = 9, y = -2, yend = 2), color = "blue", size = 1) +
  
  
  # Text labels for x, a, and x + a
  annotate("text", x = 4, y = -0.3, label = "x", color = "blue", size = 5, hjust = 1.5) +
  annotate("text", x = 9, y = -0.3, label = "y", color = "blue", size = 5, hjust = -0.2) +
  
  # Customize the plot: remove axis lines, ticks, and background
  coord_cartesian(xlim = c(0, 12), ylim = c(-5, 5))
```

::: columns
::: {.column width="50%"}
```{r, echo = F}
print(plot + geom_line(aes(x = x_values, y = crazy_values)))
```
:::

::: {.column width="50%"}
```{r, echo = F}
print(plot + geom_line(aes(x = x_values, y = truncated_normal)))
```
:::
:::



---

## The solution: Posterior upper and lower probabilities (Theorem 1) {.smaller}

-   We can define $\Pi_{\theta | Y} = \{\pi_{\theta|Y} = \int_{\Phi}\pi_{\theta|\phi}d\pi_{\phi |Y}\}$
-   For any set $D$ we can define the posterior lower probability as $$
    \underline{\pi}_{\theta|Y}(D) = \inf_{\pi_{\theta |Y} \in \Pi_{\theta|Y}} \pi_{\theta|Y}(D) = \pi_{\phi|Y}(\{\phi | IS_{\theta}(\phi) \subset D\})
    $$
-   and the posterior upper probability as $$
    \overline{\pi}_{\theta|Y}(D) = \sup_{\pi_{\theta |Y} \in \Pi_{\theta|Y}} \pi_{\theta|Y}(D) = \pi_{\phi|Y}(\{\phi | IS_{\theta}(\phi) \cap D \neq \emptyset\})
    $$
-   Note these hold for any $D$. In general the worst and best case scenario depend on the set $D$
-   $\underline{\pi}_{\theta|Y}$ is the lowest possible posterior probability assigned the event $\{\theta \in D\}$ no matter which prior is chosen by the researcher 

::: {.notes}
The assumptions that go into this theorem are basically that the probability distributions are well defined and that the maps are lower semi-continuous.

Also it's important to mention that here we are entering the world of set valued random variables. The identified set is seen as a random realisation of the data generating process.

Futhermore the second equalities are important as we can observe the reduced form parameter and therefore approximate these probabilities by calculation.
:::

---

### Some intuition for the lower probabilities

```{r, echo = F}
flat_prior <- rep(1, 100)/(9-4)
data <- data |> mutate(flat_prior = flat_prior)
colors <- c("Uniform distribution" = "green")

plot + # Vertical line for x (point on axis)
  geom_segment(aes(x = 2, xend = 2, y = -2, yend = 2), color = "red", size = 1) +
  # Vertical line for x + a (point on axis)
  geom_segment(aes(x = 3, xend = 3, y = -2, yend = 2), color = "red", size = 1) +
  annotate("text", x = 2.5, y = 2.5, label = "Hypothesis D [2,3]", size = 4, color = "red")+
  geom_rect(aes(xmin = 4, xmax = 9, ymin = 0, ymax = 2), fill = "lightblue", alpha = 0.5, title = "Uniform") +
  annotate("text", x = 8, y = 3, label = "Uniform distribution on the Identified set [4,9]", size = 5, color = "black")


#print(plot + geom_line(aes(x = x_values, y = crazy_values)))
```

---

```{r, echo = F}
flat_prior <- rep(1, 100)/(9-4)
data <- data |> mutate(flat_prior = flat_prior)
colors <- c("Uniform distribution" = "green")

plot + # Vertical line for x (point on axis)
  geom_segment(aes(x = 2, xend = 2, y = -2, yend = 2), color = "red", size = 1) +
  # Vertical line for x + a (point on axis)
  geom_segment(aes(x = 5, xend = 5, y = -2, yend = 2), color = "red", size = 1) +
  annotate("text", x = 3, y = 2.5, label = "Hypothesis D [2,5]", size = 4, color = "red")+
  
  geom_rect(aes(xmin = 5, xmax = 9, ymin = 0, ymax = 2), fill = "lightblue", alpha = 0.5, title = "Uniform") +
  annotate("text", x = 8, y = 3, label = "Uniform distribution on a subset of the IS [5,9]", size = 5, color = "black")

#print(plot + geom_line(aes(x = x_values, y = crazy_values)))
```

---

```{r, echo = F}
flat_prior <- rep(1, 100)/(9-4)
data <- data |> mutate(flat_prior = flat_prior)
colors <- c("Uniform distribution" = "green")

plot + # Vertical line for x (point on axis)
  geom_segment(aes(x = 3.5, xend = 3.5, y = -2, yend = 2), color = "red", size = 1) +
  # Vertical line for x + a (point on axis)
  geom_segment(aes(x = 10, xend = 10, y = -2, yend = 2), color = "red", size = 1) +
  annotate("text", x = 3, y = 2.5, label = "Hypothesis D [3.5,10]", size = 4, color = "red")+
  
  geom_rect(aes(xmin = 4, xmax = 9, ymin = 0, ymax = 2), fill = "lightblue", alpha = 0.5, title = "Uniform") +
  annotate("text", x = 8, y = 3, label = "Uniform distribution on [5,9]", size = 5, color = "black")

#print(plot + geom_line(aes(x = x_values, y = crazy_values)))
```

---

## Summarising information: The set of posterior means {.smaller}

-   Theorem 1 gives us lower and upper probabilities for an arbitrary event $D$, next step is to summarise the information in the class of posteriors without specifying $D$.
-   The paper proposes the set of posterior means $\{E_{\theta | Y}(\theta) | \pi_{\theta | Y} \in \Pi_{\theta | Y}\}$.
-   Theorem 2: Under measurability conditions and $E_{\phi | Y}[\sup_{\theta \in IS_{\theta}(\phi)}||\theta||] < \infty$, given that $co(IS_{\theta}(\phi))$ is the convex hull of $IS_{\theta}(\phi)$ and $E^{A}_{\phi|Y}$ is the aumann expectation with probability measure $\phi_{\phi |Y}$ we have
$$
    \{E_{\theta | Y}(\theta) | \pi_{\theta | Y} \in \Pi_{\theta | Y}\} = E^{A}_{\phi|Y}[co(IS_{\theta}(\phi))]
$$

---

## What is the Aumann expectation and what is the expectation of a random set? {.smaller}

-   To prove all the results in the paper they use random set theory, where instead of having a single outcome, you have a set of outcomes of your random variable.

-   Definition: For a random correspondence $\mathcal{X}$, $E^{A}[\mathcal{X}] = \{E(f) | f \text{ is a measurable selection of }F\}$

-   Consider $\mathcal{X} : \{1,2\} \to \mathcal{P}(\{1,2,3\}), 1 \mapsto \{1,2\}, 2 \mapsto \{2,3\}$
$$
\begin{cases}
  f_{1}(1) = 1, f_{1}(2) = 2\\
  f_{2}(1) = 1, f_{2}(2) = 3\\
  f_{3}(1) = 2, f_{3}(2) = 2\\
  f_{4}(1) = 2, f_{4}(2) = 3
\end{cases} \Rightarrow E[\mathcal{X}] = \{E(f_{1}), E(f_{2}), E(f_{3}), E(f_{4})\} \stackrel{\mathbb{P} = U\{1,2\}}{=} \{1.5, 2, 2.5\}
$$
    
---

## A little note on support functions {.smaller}
- Definition: Let $A \subseteq \mathbb{R}^{n}$, then the support function of $A$ is $\sigma_{A} : \mathbb{R}^{n} \to \mathbb{R} \cup \{+\infty\}$ defined by
$$
  \sigma_A(v) = \sup_{x \in A}\langle q,x \rangle
$$
- Why? Support functions provide a characterisation of closed convex sets = convex hulls. So instead of working over the space of all possible sets, we can work over the function space $\{f | f:\mathbb{R}^{n} \to \mathbb{R} \cup \{+\infty\}\}$, where there is a lot more theory (see linear forms).
- In the context of this paper, they come up a lot.


---

## Picture of support functions

![Left: $\sigma_{A}(p) = ||p|| h$, Right: $\sigma_{A}(p) = -||p|| h$](Support_function.PNG)

---

## Why the Aumann expectation and why the set of posterior means? {.smaller}
- Consider 1D case $\Theta \subseteq \mathbb{R}$ then 
$$
\begin{align*}
&E^{A}_{\phi |Y}[co(IS_{\theta}(\phi))] = \left[E^{A}_{\phi |Y}[l(\phi)], E^{A}_{\phi |Y}[u(\phi)]\right]\\
&l(\phi) = \inf\{\theta | \theta \in IS_{\theta}(\phi)\}, \quad u(\phi) = \sup\{\theta | \theta \in IS_{\theta}(\phi)\}
\end{align*}
$$
- In a more general setting for for $A  := IS_{\theta}(\phi)$ given $\sigma_{A}(q) :\mathbb{R}^{n} \to \mathbb{R} \cup \{+\infty\}$, we have $\sigma_{E^{A}_{\phi|Y}[co(IS_{\theta}(\phi))]} = E_{\phi |Y}[\sigma_{IS_{\theta}(\phi)}]$
- Note $\sigma_{[a,b]}(v) = \begin{cases} bv \text{ if } v >0\\ av \text{ if } v<0 \\ 0 \text{ if } v = 0\end{cases}$
- Q: Is the true parameter always going to be in this set?
 
::: {.notes}
Note: They claim that the expectation of the support function can be calculated by monte carlo, how easy that would be I do not know
:::

---

## Robust credible regions {.smaller}
- The motivation for this paper was that Frequentists and Bayesians used to disagree on inference in set identified models
- From a frequentist perspective the Bayesian 95% credible region coincides with the Frequentist 95% confidence interval. I.e. they contain the true parameter 95% of the time.
- In set identification, the Bayesian credible region concentrates *strictly within* the identified set (see Moon and Schorfheide 2012) and thus contains the true parameter all of the time for some parameters or never for other parameter values in for example the sign identified case. 
- The solution: Robust credible intervals i.e. "The sets where the posterior credibility of $\theta$ is at least $\alpha$ no matter the posterior" $C_{\alpha}$ s.t.
$$
\underline{\pi}_{\theta | Y}(C_{\alpha}) = \pi_{\phi|Y}(\{\phi | IS_{\theta}(\phi) \subset D\}) \geq \alpha
$$

---

# Asymptotic properties

---

## Consistency {.smaller}
- Given true parameter value $\phi_{0}$
- Assumption 2:
  - i) $IS_{\theta}(\phi_{0})$ is bounded and $IS_{\theta}(\cdot)$ is continuous at $\theta = \theta_{0}$ (note set continuity)
  - ii) Posterior of $\phi$ is consistent for $\phi_{0}$, $P_{Y^{\infty}|\theta_{0}}$-a.s.
  - iii) $\exists \delta > 0$ s.t. $IS_{\theta}(\phi) \in L_{1+\delta}(\pi_{\phi|Y})$
- Theorem 3: a) Under i) and ii) $\lim_{T \to \infty } \pi_{\theta| Y^{T}}(\{\phi | d_{H}(IS_{\theta}(\phi), IS_{\theta}(\phi_{0})) > \epsilon\}) = 0$
- Theorem 3: b) Suppose $\pi_{\phi}$ is non atomic then $\lim_{T \to \infty } d_{H}(E^{A}_{\phi |Y}[co(IS_{\theta}(\phi))], co(IS_{\theta}(\phi_{0}))) = 0$, $P_{Y^{\infty}|\phi_{0}}$-a.s.


::: {.notes}
A) The identified set $IS_{\theta}(\phi)$ converges to $IS_{\theta}(\phi_{0})$ under the Hausdorff Metric
B) We can use the posterior means as an approximation of the true identified set.
:::

---

## Coverage of robust credible region {.smallers}
- Asumption 3:
  - i) $IS_{\theta}(\phi)$ is $\pi_{\theta}$-a.s. closed and bounded and $IS_{\theta}(\phi_{0})$ is closed and bounded
  - ii) $C_{\alpha}$ is in the class of closed and convex sets of $\mathbb{R}^{k}$
- Assumption 4: To do with the convergence of the support function of the convex hull of the identified set to the true convex hull of the identified set.
  - The assumptions characterise the geometry of the identified set and if the support function is not differentiable at some point then the assumptions can be violated
  
  
::: {.notes}
Assumption 3 is fairly technical and not very important in practice according to the authors
Assumption 4 can be simplified in the case of a scalar value, it's basically to do with whether the estimator satisfies the Bernstein-von-Mises property. While the second bit of the assumption simplifies to whether the lower and upper probability bounds are differentiable in an open interval around the true value with non zero derivative at the true value
:::

---

## An example of a possible violation
```{r, echo = F}

square_data <- data.frame(
  x = c(0, 1, 1, 0, 0),
  y = c(0, 0, 1, 1, 0)
)

# Create a data frame for the red dot
dot_data <- data.frame(x = 0, y = 0)

# Create the plot
ggplot() +
  # Add the square (polygon)
  geom_polygon(data = square_data, aes(x = x, y = y), fill = NA, color = "black") +
  # Add the red dot at (0,0)
  geom_point(data = dot_data, aes(x = x, y = y), color = "red", size = 3) +
  # Set axis limits
  xlim(-0.5, 1.5) + ylim(-0.5, 1.5) +
  # Add labels and title
  labs(x = "x", y = "y", title = "Non differentiability of the support function of an identified set") +
  theme_minimal()

```

---

### Coverage of robust credible region 2
- Given the assumptions above hold then, $C_{\alpha}$ is an asymptotically valid confidence interval as
$$
\lim_{T \to \infty} P_{Y^{T}|\phi_{0}}(IS_{\theta}(\phi) \subset C_{\alpha}) \geq \alpha
$$
- And give that $C_{\alpha}$ satisfies $\pi_{\phi | Y^{T}}(IS_{\theta}(\phi) \subset C_{\alpha}) = \alpha$ then
$$
\lim_{T \to \infty} P_{Y^{T}|\phi_{0}}(IS_{\theta}(\phi) \subset C_{\alpha}) = \alpha
$$
::: {.notes}
The second bit shows that if you construct the credible interval to satisfy the bound with equality then asymptotically the bound will also be satisfied with equality.
:::

---

## Diagnostic tools

---

## An AR(0) example, if we have time 

---

### Thank you

---

### Sources

Li, S., Ogura, Y., Kreinovich, V. (2002). The Aumann Integral and the Conditional Expectation of a Set-Valued Random Variable. In: Limit Theorems and Applications of Set-Valued and Fuzzy Set-Valued Random Variables. Theory and Decision Library, vol 43. Springer, Dordrecht. https://doi.org/10.1007/978-94-015-9932-0_2 \# Graveyard

MAA 307: Convex optimization and optimal control. Samuel Amstutz. CMAP and Department of Applied Mathematics. Ecole polytechnique

Moon, Hyungsik Roger, and Frank Schorfheide. "Bayesian and frequentist inference in partially identified models." Econometrica 80.2 (2012): 755-782.


## A running example

-   As a running example I will be using an SVAR(0) of the form $$Ay_{t} = \epsilon_{t}, \quad y_{t}, \epsilon_{t} \in \mathbb{R}^{2}, A \in \mathcal{M}(2,2)$$
-   The reduced form VAR(0) is $y_{t} = A^{-1}\epsilon_{t} = \eta_{t}$ with $E[\eta_{t}\eta_{t}^{T}]= \Sigma =  A^{-1}(A^{-1})^{T} = (A^{T}A)^{-1}$
-   Note that $\forall Q \in \mathcal{O}(2)$, we have that $\Sigma = (A^{T}QQ^{T}A)^{-1}$, the model is set identified

```{r, echo = F}
# Now for a flat model
n <- 10000  # Number of samples
mean_flat <- 0  # Mean of the flat region
width_flat <- 3  # Width of the flat region (wide for a flat middle region)
decay_width <- 0.5  # Width of the smooth decay region

# Generate x values
x_values <- seq(-width_flat - decay_width, width_flat + decay_width, length.out = n)

# Create flat middle region (uniform distribution)
flat_region <- ifelse(abs(x_values) <= width_flat, 1, 0)

# Create smooth decay using a sigmoid function (logistic decay)
decay_left <- 1 / (1 + exp(-10 * (x_values[x_values < -width_flat] + width_flat)))
decay_right <- 1-1 / (1 + exp(-10 * (x_values[x_values > width_flat] - width_flat)))

# Combine the regions: flat in the middle and sigmoid decay on the edges
combined_distribution <- flat_region

# Combine decay regions smoothly on both ends
combined_distribution[x_values < -width_flat] <- decay_left
combined_distribution[x_values > width_flat] <- decay_right

# Plot the distribution
ggplot(data.frame(x = x_values, y = combined_distribution), aes(x = x, y = y)) +
  geom_line(color = "darkblue") +
  ylim(-0.5,1.5) +
  xlim(-4,4)+
  geom_area(fill = "lightblue", alpha = 0.5) +
  labs(title = "Set identified Middle Region",
       x = "Value", y = "Density") +
  theme_minimal() +
  theme(
    axis.text.y = element_blank(),  # Hide y-axis text
  )

```
